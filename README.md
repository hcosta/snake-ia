# 游냀 Snake IA: De Q-Learning a Deep Q-Learning

[![Python Version](https://img.shields.io/badge/python-3.11%2B-blue)]()
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)]()
![GPU Recommended](https://img.shields.io/badge/GPU-Recommended-brightgreen?logo=nvidia&logoColor=white)
![Deep RL Project](https://img.shields.io/badge/Deep%20RL-Project-blueviolet?logo=tensorflow&logoColor=white)

> Un d칤a me aburr칤a y entren칠 una IA... 춰Y la cosa se puso interesante! Aqu칤 explico la experiencia.

<table>
  <tr>
    <td align="center">Q-Learning</td>
    <td align="center">Deep Q-Learning</td>
  </tr>
  <tr>
    <td><img src="./docs/demo-QL.gif" alt="Demostraci칩n del Agente Q-Learning"/></td>
    <td><img src="./docs/demo-DQN.gif" alt="Demostraci칩n del Agente Deep Q-Learning (versi칩n reciente)"/></td>
  </tr>
</table>

## Contenido

- [游냀 Snake IA: De Q-Learning a Deep Q-Learning](#-snake-ia-de-q-learning-a-deep-q-learning)
  - [Contenido](#contenido)
  - [El Proyecto](#el-proyecto)
  - [쮼n qu칠 consiste esta pr치ctica? (para dummies)](#en-qu칠-consiste-esta-pr치ctica-para-dummies)
  - [Evoluci칩n, Experiencia y Observaciones](#evoluci칩n-experiencia-y-observaciones)
    - [Q-Learning Cl치sico ("La Chuleta")](#q-learning-cl치sico-la-chuleta)
    - [Deep Q-Learning ("El Cerebro Artificial" y sus Versiones)](#deep-q-learning-el-cerebro-artificial-y-sus-versiones)
    - [Resumen Comparativo de Versiones](#resumen-comparativo-de-versiones)
  - [Reflexiones y Futuros Pasos](#reflexiones-y-futuros-pasos)
  - [C칩mo Probar los Agentes](#c칩mo-probar-los-agentes)
    - [Requisitos previos](#requisitos-previos)
    - [Instalaci칩n](#instalaci칩n)

## El Proyecto

Este proyecto naci칩 de la curiosidad y el desaf칤o personal de entrenar una Inteligencia Artificial para dominar el cl치sico juego de Snake. A lo largo de este viaje, he explorado, comparado y evolucionado diferentes enfoques del aprendizaje por refuerzo:

1.  **Primer Intento: Aprendizaje con una "Chuleta" (Q-Learning Cl치sico)**
2.  **Iteraciones con un "Cerebro" para la Serpiente (Deep Q-Learning con Redes Neuronales)**: Esta fase ha tenido m칰ltiples versiones, cada una construyendo sobre la anterior.

El objetivo final es observar la evoluci칩n del aprendizaje, las limitaciones de cada m칠todo, los desaf칤os t칠cnicos y, por supuesto, 춰ver hasta d칩nde puede llegar cada IA!

## 쮼n qu칠 consiste esta pr치ctica? (para dummies)

Imagina que queremos ense침arle a una serpiente digital a jugar al Snake.

1.  **El M칠todo de la "Chuleta" (Q-Learning Cl치sico)**:
    * Al principio, es como si le di칠ramos a la serpiente una enorme hoja de trucos. Por cada situaci칩n posible en el juego (d칩nde est치 la comida, d칩nde est치n los obst치culos, etc.), la IA anota qu칠 tan buena o mala es cada posible acci칩n (moverse arriba, abajo, izquierda o derecha).
    * Fui ajustando c칩mo la IA "ve칤a" el tablero y qu칠 consideraba un "premio" (comer la fruta) o un "castigo" (chocar). Tambi칠n le ense침칠 a reconocer peligros b치sicos.
    * Aunque la serpiente aprendi칩 bastante, lleg칩 un punto en que no mejoraba m치s. Se quedaba atascada en sus propios enredos. La "chuleta" se volv칤a demasiado grande e inmanejable para todas las sutilezas del juego.

2.  **El M칠todo del "Cerebro Artificial" (Deep Q-Learning con Redes Neuronales)**:
    * Como la "chuleta" no fue suficiente, decid칤 darle a la serpiente un "cerebro" artificial. Este cerebro usa redes neuronales (con TensorFlow) y es capaz de aprender patrones complejos y tomar decisiones m치s inteligentes, incluso en situaciones no vistas id칠nticamente antes.
    * Este "cerebro" aprende jugando much칤simas partidas. Para optimizar su aprendizaje, he implementado y refinado varias t칠cnicas a lo largo de diferentes versiones:
        * **Memoria de Experiencias (Replay Memory)**: Guarda lo que hizo y qu칠 pas칩 despu칠s, para repasar y aprender de aciertos y errores.
        * **Red Neuronal Objetivo (Target Network)**: Una "copia de seguridad" de la red principal que se actualiza m치s despacio, ayudando a estabilizar el aprendizaje.
        * **Entrenamiento "Headless"**: Reorganic칠 el c칩digo para entrenar la IA sin necesidad de verla jugar (mucho m치s r치pido) y desarroll칠 interfaces separadas para visualizaci칩n (gr치fica con Arcade y en modo texto con `curses`).

## Evoluci칩n, Experiencia y Observaciones

### Q-Learning Cl치sico ("La Chuleta")

* **Desaf칤os Iniciales**: Ajustar la exploraci칩n fue clave; demasiada aleatoriedad limitaba el progreso.
* **Entrenamiento y Rendimiento**: A los 10,000 episodios la IA se estanc칩 asi que decid칤 hacer un ciclo m치s largo de 200,000 episodios (aprox. 4 horas), pero la mejora fue apenas significativa, alcanzado una **puntuaci칩n media de 21~22 puntos**, con picos ocasionales cercanos a 40. 
* **El Gran L칤mite**: La incapacidad para resolver eficazmente el problema de **quedar encerrado en s칤 mismo** fue su principal tal칩n de Aquiles. La planificaci칩n a largo plazo era demasiado compleja para la tabla Q.
* **Conclusi칩n sobre Q-Learning**: Aunque Snake parece simple, requiere una estrategia considerable. Q-Learning, al memorizar estados, no es ideal para esta complejidad, lo que motiv칩 la transici칩n a redes neuronales.

### Deep Q-Learning ("El Cerebro Artificial" y sus Versiones)

El camino con Deep Q-Learning ha sido un proceso iterativo de experimentaci칩n, optimizaci칩n y aprendizaje, tanto para la IA como para m칤.

* **Retos T칠cnicos Iniciales (Comunes a varias versiones)**:
    * **Configuraci칩n de TensorFlow y GPU**: Entrenar con CPU ocupaba muchos recursos, entre 90 y 99%. La soluci칩n m치s efectiva fue usar contenedores Docker con entornos TensorFlow-GPU preconfigurados por Nvidia, reduciendo significativamente el consumo de la CPU a un 15-20%.
    * **Entrenamiento Headless y M칰ltiples Interfaces**: La necesidad de entrenar en entornos sin gr치ficos (como Docker) llev칩 a refactorizar el c칩digo para separar la l칩gica del juego de la UI. Esto permiti칩 un entrenamiento eficiente y la creaci칩n de interfaces con Arcade (gr치fica) y `curses` (terminal).

* **Hitos y Aprendizajes por Versi칩n (Resumen)**:

    * **Versi칩n Inicial a v4**: Primeras implementaciones del DQN, ajustes b치sicos de hiperpar치metros, y soluci칩n de los retos t칠cnicos de GPU y entrenamiento headless. Se observ칩 potencial, pero el rendimiento a칰n era err치tico.

    * **Versi칩n 5 ("Optimizaciones y Paralelismo")**:
        * Se introdujo **Numba**, un compilador Just-In-Time (JIT) para Python, que traduce funciones de Python y NumPy a c칩digo m치quina optimizado. Se aplic칩 para acelerar c치lculos cr칤ticos como la detecci칩n de colisiones.
        * Se experiment칩 con el m칩dulo `multiprocessing` de Python para intentar paralelizar partes del entrenamiento y aprovechar m칰ltiples n칰cleos de la CPU. Sin embargo, la complejidad a침adida no se tradujo en una mejora clara del rendimiento para este caso de uso espec칤fico y se descart칩 para simplificar.

    * **Versi칩n 6 ("Explorando Nuevos Estados")**:
        * Se prob칩 a침adir una nueva caracter칤stica al estado del agente: la "Accesibilidad de la Cola Despu칠s de Comer". La idea era darle a la serpiente informaci칩n sobre si, despu칠s de comer una fruta, su propia cola bloquear칤a un camino vital.
        * Este experimento result칩 ser **contraproducente**. A침adir esta informaci칩n, que adem치s requer칤a c치lculos adicionales (como un BFS), pareci칩 confundir al agente o a침adir ruido al estado, llevando a peores resultados de aprendizaje. Fue una lecci칩n valiosa sobre c칩mo m치s informaci칩n no siempre es mejor.

    * **Versi칩n 7 ("Alineaci칩n con Investigaci칩n y Mejoras Dr치sticas")**:
        * Se realiz칩 una investigaci칩n m치s profunda de implementaciones de Snake IA existentes y *papers* acad칠micos (como el de Sourena Khanzadeh) para comparar arquitecturas y par치metros.
        * **Cambios Clave Inspirados en la Investigaci칩n**:
            * Se **redujo el espacio de estados de 12 (en v6) de nuevo a 11 par치metros booleanos simples**, eliminando la "accesibilidad de la cola" y otros estados complejos, para coincidir con configuraciones probadas.
            * Se **cambi칩 el tama침o del tablero de 12x12 a 20x20**, proporcionando un entorno de aprendizaje m치s extenso y comparable al del informe de referencia.
            * Se ajust칩 el **decaimiento de 칠psilon de multiplicativo a lineal**, permitiendo una fase de exploraci칩n m치s controlada y prolongada.
            * Se refinaron otros hiperpar치metros (gamma, batch size, frecuencia de actualizaci칩n de la red objetivo) para alinearlos con las mejores pr치cticas observadas.
        * **Resultados**: Estos ajustes produjeron una **mejora muy significativa**. El agente comenz칩 a mostrar un aprendizaje mucho m치s r치pido y eficiente. Por ejemplo, tras unos 1000 episodios de entrenamiento en esta configuraci칩n, el agente ya alcanzaba una **puntuaci칩n media de alrededor de 23-24 puntos**, una mejora dr치stica en comparaci칩n con las decenas de miles que se necesitaban antes para progresos menores o con Q-Learning para conseguir los mismos resultados.

* **Evoluci칩n General del Aprendizaje (con DQN v7 y posteriores refinamientos)**:
    * El agente DQN, especialmente a partir de la v7, muestra un rendimiento muy prometedor. Aunque el n칰mero total de episodios de entrenamiento a칰n puede ser menor que el del Q-Learning en algunas comparativas, la **eficiencia (puntuaci칩n/episodio) y la tendencia ascendente son claramente superiores**.
    * El aprendizaje sigue un patr칩n de "dientes de sierra": ciclos de mejora, seguidos de mesetas donde parece consolidar lo aprendido, para luego volver a escalar.
    * El principal desaf칤o sigue siendo perfeccionar las estrategias para evitar auto-colisiones complejas a largo plazo, pero la base actual es mucho m치s s칩lida.

### Resumen Comparativo de Versiones

| Versi칩n   | Puntuaci칩n | Episodios | Comentarios                                         |
|-----------|------------|-----------|-----------------------------------------------------|
| QL v1     | ~20        | 10,000    | A partir de aqu칤 se estanca                          |
| QL v2     | ~20        | 10,000    | Separaci칩n de colisi칩n en dos partes, poca mejora  |
| DQN v4    | ~20        | 10,000    | Diferentes ajustes b치sicos, rendimiento err치tico   |
| DQN v5    | ~20        | 10,000    | Optimizaci칩n con Numba y pruebas de paralelismo    |
| DQN v6    | ~10        | 10,000    | Se a침ade un 12췈 par치metro; resultado contraproducente |
| DQN v7    | ~24        | 1,000     | Vuelta a 11 par치metros y optimizaci칩n inspirada en papers |
| DQN v8    | ~28        | 1,000     | Ajuste del min epsilon, recompensas e hiperpar치metros |
| DDQN v9   | ~28        | 300       | Implementaci칩n algoritmo Double DQN, mismos ajustes  |

> Las puntuaciones pueden variar entre ejecuciones, estas cifras representan tendencias generales observadas.

## Reflexiones y Futuros Pasos 

Este proyecto, aunque ha avanzado considerablemente, abre la puerta a numerosas exploraciones y plantea preguntas interesantes sobre los l칤mites y la filosof칤a del entrenamiento de IA a nivel individual.

* **Limitaciones de Recursos y Escala**:
    * Alcanzar el rendimiento de modelos entrenados a gran escala (con millones o miles de millones de interacciones, como se ve en investigaciones con grandes recursos computacionales) es un desaf칤o enorme para proyectos personales. La disponibilidad de GPUs potentes y el tiempo de entrenamiento son factores limitantes.
    * Si bien la optimizaci칩n del c칩digo (como el uso de Numba o la mejora de la eficiencia de TensorFlow) ayuda, la cantidad bruta de "experiencia" (episodios jugados) sigue siendo un factor dominante en el aprendizaje por refuerzo profundo.

* **쯄치s Optimizaci칩n vs. "Pureza" del Aprendizaje?**:
    * **Ingenier칤a de Recompensas Adicional (Reward Shaping)**: Se podr칤a experimentar con recompensas m치s gradadas (ej., peque침as recompensas por acercarse a la comida, penalizaciones por acercarse a las paredes antes de una colisi칩n inminente). Sin embargo, como se observ칩 en la investigaci칩n de referencia y en la literatura de RL, esto es un arma de doble filo. Puede guiar el aprendizaje, pero tambi칠n puede llevar a que la IA "piratee" el sistema de recompensas, aprendiendo comportamientos sub칩ptimos que maximizan estas recompensas intermedias en lugar del objetivo real (sobrevivir y comer).
    * **Incorporaci칩n de Conocimiento Expl칤cito (Heur칤sticas o Patrones)**: 쯉er칤a beneficioso "ense침arle" a la IA patrones de movimiento conocidos como estrategias de zig-zag o "box patterns" para llenar el espacio de forma segura?
        * **Argumento a favor**: Podr칤a acelerar el aprendizaje de estrategias de supervivencia a largo plazo y alcanzar puntuaciones m치s altas m치s r치pidamente.
        * **Argumento en contra (la "pureza")**: El objetivo fundamental del aprendizaje por refuerzo es que el agente descubra estas estrategias por s칤 mismo a partir de la interacci칩n con el entorno y las recompensas. Introducir heur칤sticas podr칤a considerarse "hacer trampa" o limitar la capacidad del agente para encontrar soluciones novedosas o incluso superiores que un humano no habr칤a dise침ado.
    * **El Equilibrio**: La filosof칤a adoptada hasta ahora ha sido proporcionar al agente la informaci칩n de estado necesaria y un sistema de recompensas claro, permiti칠ndole derivar la pol칤tica de acci칩n por s칤 mismo. Este enfoque busca un aprendizaje m치s general y aut칩nomo.

* **Posibles L칤neas de Trabajo Futuras (si los recursos y el tiempo lo permiten)**:
    * **Entrenamiento Extensivo**: Continuar el entrenamiento del agente DQN v7 (o versiones posteriores) durante un n칰mero mucho mayor de episodios (decenas de miles o cientos, si es factible) para ver hasta d칩nde puede llegar su rendimiento con la configuraci칩n actual.
    * **Ajuste Fino de Hiperpar치metros**: Realizar una b칰squeda m치s sistem치tica de hiperpar치metros (tasa de aprendizaje, arquitectura de la red, par치metros de decaimiento de 칠psilon, tama침o de la memoria de repetici칩n) podr칤a exprimir un rendimiento adicional.
    * **Algoritmos de RL M치s Avanzados**: Explorar otros algoritmos de Deep RL como Dueling DQN, Double DQN, A2C/A3C, o PPO, que han demostrado mejoras en otros dominios.
    * **An치lisis Profundo del Comportamiento**: Utilizar herramientas de visualizaci칩n de activaciones de la red o mapas de prominencia (saliency maps) para intentar comprender mejor qu칠 est치 "pensando" la IA y por qu칠 toma ciertas decisiones, especialmente en situaciones complejas.

Este proyecto subraya que, incluso en un juego aparentemente simple como Snake, los desaf칤os del aprendizaje por refuerzo profundo son considerables y ofrecen un terreno f칠rtil para la experimentaci칩n y la reflexi칩n continua.

## C칩mo Probar los Agentes

Aseg칰rate de tener Python 3.11 o superior (o ajusta la versi칩n en `Pipfile`).

### Requisitos previos

- Python 3.11+
- Pipenv (`pip install pipenv`)
- **Recomendado para entrenar DQN**:
  - NVIDIA GPU con drivers actualizados
  - CUDA Toolkit 11.8
  - cuDNN 8.6

### Instalaci칩n

1.  **Clonar el repositorio (si a칰n no lo has hecho):**
    ```bash
    git clone https://github.com/hcosta/snake-ia.git
    cd snake-ia
    ```

2.  **Instalar las dependencias usando Pipenv (en el directorio principal):**
    ```bash
    pipenv install
    ```

3.  **Ejecutar el Agente Q-Learning (Ejemplo):**
    ```bash
    cd snake-QL
    pipenv run python trainer_v2.py --play --play-speed=0.02 
    ```

4.  **Entrenar el Agente Q-Learning (Ejemplo):**
    ```bash
    cd snake-QL
    pipenv run python trainer_v2.py --no-visualize --reset --episodes=10000
    ```

5.  **Ejecutar el Agente DQN (Versi칩n 7 o la m치s reciente):**
    ```bash
    cd snake-DQN
    pipenv run python trainer_v7.py --play --play-speed=0.02
    ```

6.  **Entrenar el Agente DQN (Versi칩n 7 o la m치s reciente):**
    ```bash
    cd snake-DQN
    pipenv run python trainer_v7.py --episodes=1000 --reset
    ```

7.  **Jugar al Snake Manualmente (sin IA):**
    * **Versi칩n Gr치fica (Arcade):**
    ```bash
    cd snake-DQN
    pipenv run python snake_ui_v10.py
    ```
    * **Versi칩n en Terminal (Curses):**
    ```bash
    cd snake-DQN
    pipenv run python snake_shell_v10.py
    ```