# Snake IA (Q-Learning vs Deep Q-Learning)

> Un d√≠a me aburr√≠a y entren√© una IA (para jugar Snake). ¬°Y la cosa se puso interesante!

<table>
  <tr>
    <td align="center">Q-Learning en Acci√≥n</td>
    <td align="center">Deep Q-Learning en Acci√≥n</td>
  </tr>
  <tr>
    <td><img src="./docs/demo-QL.gif" alt="Demostraci√≥n del Agente Q-Learning"/></td>
    <td><img src="./docs/demo-DQN.gif" alt="Demostraci√≥n del Agente Deep Q-Learning"/></td>
  </tr>
</table>

## üéØ El Proyecto

Este proyecto naci√≥ de la curiosidad y el desaf√≠o personal de entrenar una Inteligencia Artificial para dominar el cl√°sico juego de Snake. Para ello, he explorado y comparado dos enfoques principales del aprendizaje por refuerzo:

1.  **Primer Intento: Aprendizaje con una "Chuleta" (Q-Learning Cl√°sico)**
2.  **Segundo Intento: Un "Cerebro" para la Serpiente (Deep Q-Learning con Redes Neuronales)**

El objetivo final es observar la evoluci√≥n del aprendizaje, las limitaciones de cada m√©todo y, por supuesto, ¬°ver hasta d√≥nde puede llegar cada IA!

## ü§î ¬øEn qu√© consiste esta pr√°ctica? (La explicaci√≥n para Dummies)

Imagina que queremos ense√±arle a una serpiente digital a jugar al Snake.

1.  **El M√©todo de la "Chuleta" (Q-Learning Cl√°sico)**:
    * Al principio, es como si le di√©ramos a la serpiente una enorme hoja de trucos. Por cada situaci√≥n posible en el juego (d√≥nde est√° la comida, d√≥nde est√°n los obst√°culos, etc.), la IA anota qu√© tan buena o mala es cada posible acci√≥n (moverse arriba, abajo, izquierda o derecha).
    * Fui ajustando c√≥mo la IA "ve√≠a" el tablero y qu√© consideraba un "premio" (comer la fruta) o un "castigo" (chocar). Tambi√©n le ense√±√© a reconocer peligros b√°sicos, como meterse en un callej√≥n sin salida.
    * Aunque la serpiente aprendi√≥ bastante, lleg√≥ un punto en que no mejoraba m√°s. Se quedaba atascada en sus propios enredos o no sab√≠a c√≥mo salir de situaciones complicadas. La "chuleta" se volv√≠a demasiado grande para todas las posibilidades del juego.

2.  **El M√©todo del "Cerebro Artificial" (Deep Q-Learning con Redes Neuronales)**:
    * Como la "chuleta" no fue suficiente, decid√≠ darle a la serpiente un peque√±o "cerebro" artificial. Este cerebro est√° hecho con redes neuronales (usando una herramienta popular llamada TensorFlow) y es mucho m√°s listo. Puede aprender patrones complejos y tomar decisiones inteligentes, incluso en situaciones que no ha visto antes de forma id√©ntica.
    * Este "cerebro" aprende jugando much√≠simas partidas. Para que aprenda bien, us√© algunas t√©cnicas especiales:
        * Tiene una **memoria** donde guarda lo que hizo y qu√© pas√≥ despu√©s, para poder repasar y aprender de sus aciertos y errores.
        * Tiene una especie de **"copia de seguridad" de s√≠ mismo** que se actualiza m√°s despacio, ayudando a que el aprendizaje sea m√°s estable.
    * Tambi√©n reorganic√© el c√≥digo del juego para poder entrenar a la IA sin necesidad de verla jugar todo el tiempo (lo que se llama entrenamiento "headless", que es mucho m√°s r√°pido) y para poder tener diferentes formas de verla jugar (una con gr√°ficos y otra en modo texto).
    * ¬°Y las sensaciones son muy buenas! Este "cerebro" parece estar aprendiendo a no quedarse atrapado tan f√°cilmente y a desarrollar estrategias m√°s astutas.

## üìä Experiencia y Observaciones

### Q-Learning Cl√°sico ("La Chuleta")

* **Entrenamiento y Rendimiento**: Despu√©s de un entrenamiento exhaustivo de aproximadamente 200,000 episodios (unas 4 horas de procesamiento), el agente de Q-Learning logr√≥ estabilizarse con una **puntuaci√≥n media de alrededor de 25-27 puntos**. En ocasiones puntuales, pod√≠a alcanzar puntuaciones significativamente m√°s altas (picos cercanos a 40).
* **Desaf√≠os Iniciales**: Un primer obst√°culo fue ajustar la exploraci√≥n. Si el agente exploraba demasiado durante mucho tiempo, sus movimientos aleatorios le imped√≠an superar los 15 puntos. Reducir la tasa de exploraci√≥n a largo plazo fue clave para mejorar.
* **El Gran L√≠mite**: El principal problema insuperable para este modelo fue su incapacidad para resolver eficazmente el problema de **quedar encerrado en s√≠ mismo**. La serpiente aprend√≠a a evitar las paredes y a buscar la comida, pero la planificaci√≥n a largo plazo para no crear bucles fatales resultaba demasiado compleja para la tabla Q.
* **Conclusi√≥n sobre Q-Learning**: Aunque el juego *Snake* parece simple inicialmente (ir hacia la comida), requiere una estrategia considerable a largo plazo para la supervivencia. Los sistemas Q-Learning cl√°sicos, al depender de memorizar cada estado, no parecen ser la herramienta m√°s adecuada para dominar esta complejidad inherente, lo que me motiv√≥ a explorar las redes neuronales.

<p align="center">
  <img src="./docs/demo-QL-Log.gif" alt="isualizaci√≥n del agente QL en modo juego (sin entrenamiento)"/>
  <em>Visualizaci√≥n del agente QL en modo juego (sin entrenamiento).</em>
</p>

### Deep Q-Learning ("El Cerebro Artificial")

* **Retos T√©cnicos Iniciales**:
    * **Configuraci√≥n de TensorFlow y GPU**: El primer escollo fue el rendimiento. Entrenar la red neuronal usando solo la CPU era extremadamente lento. Intent√© configurar TensorFlow para usar la GPU (con CUDA) de forma nativa en Windows, pero no fue sencillo. La soluci√≥n vino al utilizar contenedores Docker con entornos de TensorFlow preconfigurados por Nvidia, lo que permiti√≥ acelerar significativamente el proceso.
    * **Entrenamiento Headless**: Los contenedores Docker, por defecto, carecen de entorno gr√°fico, lo que imped√≠a ejecutar la visualizaci√≥n del juego. Aunque inicialmente us√© el comando `xvfb-run` como una soluci√≥n temporal para simular un entorno gr√°fico, finalmente opt√© por una soluci√≥n m√°s robusta: refactorizar el c√≥digo para separar completamente la l√≥gica del juego de la interfaz gr√°fica. Esto permiti√≥ un entrenamiento "headless" eficiente y la creaci√≥n de m√∫ltiples interfaces (una con Arcade y otra en modo texto para la terminal, ¬°gracias `Gemini` por la ayuda con la versi√≥n `curses`!).
* **Evoluci√≥n del Aprendizaje**:
    * Tras aproximadamente 8,500 episodios de entrenamiento (unas 10-12 horas de procesamiento con GPU, aunque con cierto cuello de botella debido a la naturaleza de Python y la comunicaci√≥n con la GPU), el agente DQN est√° mostrando un rendimiento muy prometedor. En las √∫ltimas fases del entrenamiento, est√° promediando de forma estable alrededor de **15-17 puntos** (media de los √∫ltimos 100 episodios), con picos individuales que superan los 30 puntos (¬°e incluso llegando a 41 en pruebas!).
    * Es importante destacar que, aunque la media de entrenamiento actual del DQN es inferior a la media final del Q-Learning, el DQN lo ha logrado con **much√≠simos menos episodios de entrenamiento** (8.5k vs 200k) y sigue mostrando una clara tendencia ascendente.
    * El aprendizaje sigue un patr√≥n de "dientes de sierra": hay ciclos de mejora, seguidos de peque√±as bajadas o mesetas donde parece consolidar lo aprendido, para luego volver a escalar y superar el rendimiento anterior. Por ejemplo, tard√≥ unos 3,500 episodios en promediar consistentemente 5 puntos, pero la progresi√≥n se ha acelerado notablemente despu√©s.
* **Comparativa Actual y Pr√≥ximos Pasos**:
    * Aunque el Q-Learning, con su vasto entrenamiento, todav√≠a puede mostrar una media ligeramente superior en tandas de prueba cortas, el **potencial y la eficiencia de aprendizaje del DQN son claramente superiores**. Ya iguala e incluso supera los picos de rendimiento del Q-Learning con una fracci√≥n del entrenamiento.
    * El principal desaf√≠o para el DQN sigue siendo perfeccionar las estrategias para evitar auto-colisiones a largo plazo. Continuar√© el entrenamiento hasta los 100,000 episodios (o m√°s si sigue mejorando) para observar si puede superar consistentemente al Q-Learning y dominar este aspecto del juego. No descarto reiniciar el entrenamiento con hiperpar√°metros ajustados si se observa un estancamiento persistente m√°s adelante, pero por ahora, ¬°la progresi√≥n es alentadora!

<p align="center">
  <img src="./docs/shell-DQN.png" alt="Logs del entrenamiento inicial"/>
  <em>Logs del entrenamiento tras 8.500 episodios.</em>
</p>

<p align="center">
  <img src="./docs/plot-DQN.png" alt="Gr√°fico de recompensas del entrenamiento DQN" width="70%"/>
  <em>Curva de aprendizaje del agente DQN.</em>
</p>

<p align="center">
  <img src="./docs/demo-DQN-Log.gif" alt="Visualizaci√≥n del agente DQN en modo juego (sin entrenamiento)"/>
  <em>Visualizaci√≥n del agente DQN en modo juego (sin entrenamiento).</em>
</p>

## üöÄ C√≥mo Probar los Agentes

* **Instalar las dependencias en un entorno virtual, se requiere python 3.11 o editar `python_version` del `Pipfile` manualmente con la versi√≥n del sistema**:
    ```bash
    pip install pipenv  # si no lo tienes
    pipenv install
    ```
* **Ejecutar el Agente Q-Learning**:
    ```bash
    cd snake-QL
    pipenv run python trainer_v2.py --play --play-speed=0.02
    ```
* **Ejecutar el Agente DQN**:
    ```bash
    cd snake-DQN
    pipenv run python trainer_v6_parallel.py --play --play-speed=0.02
    ```
* **Jugar al Snake sin IA en ventana (con Python Arcade)**:
    ```bash
    cd snake-DQN
    pipenv run python snake_ui_v10.py  
    ```
* **Jugar al Snake sin IA en la shell (con m√≥dulo Curses)**:
    ```bash
    cd snake-DQN
    pipenv run python snake_shell_v10.py  
    ```